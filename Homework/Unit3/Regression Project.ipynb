{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework 3:  Regression Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your homework assignment will be to synthesize the lessons taught in Unit 3, and present a coherent walk through of how you approached the modeling process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What You Will Turn In:**\n",
    "\n",
    "A jupyter notebook with code and commentary that walks us through the following:\n",
    "\n",
    " - Exploratory Data Analysis on the original data\n",
    "   - What are general patterns within the target variable? \n",
    "   - What relationships can we deduce from the features in X and how they relate to one another?  How they impact y?\n",
    "   \n",
    " - What were some of the challenges in dealing with this dataset and why?\n",
    " - What cross validation strategy did you use and why?\n",
    "   - How did you interpret their results?\n",
    "   - What did you change as a result?\n",
    "   - Did changing the number of folds have any measurable impact on your scores?\n",
    " - The use of pipelines to streamline your data processing and ensure correct alignment between training and test sets\n",
    " - Strategies you used to try and improve your score (it's okay if they didn't work -- just show us what you tried to do and why)\n",
    " - What features ended up having important causal impact on the target variable?  Can you demonstrate this graphically?\n",
    " - How did you choose your model parameters?\n",
    " - How did you validation predictions compare with your test set predictions?  Can you visualize this graphically?\n",
    "  \n",
    "The end result should be a coherent walk through of how you approached the problem and developed a coherent solution to model your data.\n",
    "\n",
    "Some bonus tasks you could take on:  \n",
    "\n",
    " - Adding more functionality to your pipelines.  In class, we used the category_encoder + gbm.  Could you add something like the missing value imputer to automate the entire process?  More info is found here:  https://scikit-learn.org/stable/modules/impute.html\n",
    " - Could you cluster your samples to add more predictive value to your dataset?  Clustering your samples is a common way to create a column that adds a lot of predictive power that's different from everything else.  The API for scikit-learn is the same across all algorithms, so you could `fit`, `score`, and `predict` the same way with a clustering method as you would with a GBM.  The most commonly used clustering technique is KMeans, which measures the 'distance' between each sample and assigns samples to clusters depending on which cluster they're closest to.  In this case you would want to cross validate the ideal number of clusters, and create a column that's the predicted cluster each sample would belong to.  Documentation can be found here:  https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html **hint:** You should one-hot encode your data and use the `standardscaler()` data to prep your data for this.\n",
    " - The `bikeshare` dataset makes use of a date column.  There are many specialized versions of KFold, one of which is Time Series Splitting.  This would split the data in a way that was described in the cross-validation class, with each validation set coming after the previous training sets.  Could you make use of this in your modeling?  You can find it here:  https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datasets You Can Work With:**\n",
    "\n",
    "Here are the list of the current datasets you have at your disposal:\n",
    "\n",
    "`housing.csv` **beginner difficulty** -- this is the boston housing dataset that was used in class.  It contains 13 columns, and the target variable would be `PRICE`.  It states the sale price, and columns associated with the house's location, zoning details, and physical characteristics.  It has no missing values, no time column, and no categorical variables, so it's the most straight forward to work with.\n",
    "\n",
    "`bikeshare.csv` **intermediate difficulty** -- this is the dataset that was part of your bonus assignment.  It represents the number of bike rentals every hour in Manhattan during the course of several years.  This dataset is a **time series**, so it's important to make judicious use of time-based data, and to make sure you cross-validate your results sequentially.\n",
    "\n",
    "`iowa_mini.csv` **intermediate difficulty** -- this is the dataset that we've worked on throughout class for the past two weeks. It has a few outliers within it, as well as some missing values that make it a bit challenging. It's a good idea for people who feel most comfortable continuing what was done in class.\n",
    "\n",
    "`iowa_full.csv` **advanced difficulty** -- this is the complete iowa dataset, which has a total of 80+ columns.  Most of these are redundant, but deciphering how to best make use of them is a lot more work than the other files listed here.  With this dataset, expect to spend a lot of time cleaning your data, and deciphering how different columns ought to be encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
